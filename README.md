# Natural Language Video Query System

## ğŸ“š Proof of Concept

A working back-end focused prototype demonstrating **natural language search** through video content by:

1. Converting video frames to vector embeddings (CLIP)
2. Storing embeddings in a vector database (ChromaDB)
3. Matching text queries against frames using semantic similarity

## ğŸ” Core Innovation

**You can search videos using everyday language**:

- "A person getting out of a car"
- "Someone wearing a red shirt walking left"
- "Close-up of a dog's face"

_Note: Accuracy depends on CLIP's understanding of your query and video content quality_

## ğŸŒ Real-World Applications

This technology could help with:

**Security & Surveillance**:

- "Show me all people wearing red jackets in last week's footage"
- "Find every time someone entered the back door after hours"

**Media Production**:

- "Locate all smiling faces in interview footage"
- "Find establishing shots of city skylines"

**Personal Video Libraries**:

- "When did we open Christmas presents last year?"
- "Find clips of Sarah playing soccer"

_Current prototype limitations mean these would require scaling improvements_

## ğŸ“¸ Interface Preview

### Video Upload

![Upload Interface](screenshots/1.png)  
_Uploading a video file through the Streamlit interface_

### Search Results

![Search Results](screenshots/2.png)  
_Example search results showing matching frames with confidence scores_

## ğŸ› ï¸ Technical Implementation

### How It Works

1. **Frame Processing**:

   - Extracts 3 frames/second using OpenCV
   - Generates 512-dimension CLIP embeddings
   - Stores vectors + metadata in ChromaDB

2. **Multimodal Query Handling**:
   - Converts text to embedding using same CLIP model
   - Finds most similar frames via cosine similarity
   - Returns ranked results with confidence scores

### Technology Stack

- **Backend**: FastAPI + Pydantic
- **Embeddings**: CLIP-ViT-B/32
- **Vector DB**: ChromaDB (in-memory)
- **Frontend**: Streamlit

## ğŸ§ª Testing Coverage

- Frame extraction reliability
- Embedding generation consistency
- Basic search functionality
- API response validation

```bash
pytest tests/  # Run all tests
```

## ğŸš€ Setup & Usage

1. Install requirements:

   ```bash
   pip install fastapi streamlit opencv-python chromadb transformers torch
   ```

2. Run system:

   ```bash
   uvicorn api:app --reload & streamlit run app.py
   ```

3. Try searching:
   - Upload a video (currently set to under 200MB)
   - Wait for processing
   - Enter natural language queries

## âš ï¸ Current Constraints

1. **Performance**:

   - No batch processing
   - 3 FPS fixed sampling rate

2. **Accuracy**:
   - Limited by CLIP model capabilities
   - No temporal understanding
   - Basic confidence scoring

## ğŸ“‚ Code Structure

```
.
â”œâ”€â”€ api.py                # Main API endpoints
â”œâ”€â”€ app.py                # Streamlit interface
â”œâ”€â”€ core/                 # Core functionality
â”‚   â”œâ”€â”€ database.py       # ChromaDB operations
â”‚   â”œâ”€â”€ embedding.py      # CLIP embeddings
â”‚   â”œâ”€â”€ models.py         # Data models
â”‚   â””â”€â”€ processor.py      # Video processing
â”œâ”€â”€ tests/                # Test cases
â””â”€â”€ storage/              # Temporary files
```

## ğŸ”® Next Steps

1. Better frame sampling strategies
2. Query refinement options
3. Basic temporal search support
4. Improved results visualization

---

_This prototype demonstrates the viability of natural language video search using current ML techniques. Contributions welcome._
